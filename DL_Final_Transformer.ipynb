{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Final Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuinnStraus/CSCI-1470-final/blob/main/DL_Final_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypianoroll"
      ],
      "metadata": {
        "id": "ahIhNfAXm0nT",
        "outputId": "6e37d690-9322-4afb-9e00-a67e566c46b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypianoroll\n",
            "  Downloading pypianoroll-1.0.4-py3-none-any.whl (26 kB)\n",
            "Collecting pretty-midi>=0.2.8\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from pypianoroll) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pypianoroll) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.7/dist-packages (from pypianoroll) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->pypianoroll) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->pypianoroll) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->pypianoroll) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->pypianoroll) (3.0.6)\n",
            "Collecting mido>=1.1.16\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty-midi>=0.2.8->pypianoroll) (1.15.0)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591953 sha256=6e92aded579b7b1e64607ec193b19591bba9f98ccae2544082c420b2858d62d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/74/7c/a06473ca8dcb63efb98c1e67667ce39d52100f837835ea18fa\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: mido, pretty-midi, pypianoroll\n",
            "Successfully installed mido-1.2.10 pretty-midi-0.2.9 pypianoroll-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT0_TItcESTP"
      },
      "source": [
        "import pypianoroll\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7Fi86GSEMgb",
        "outputId": "9ea7a267-8244-484b-fa6b-af93d4aefedc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b8w98EgLLUV"
      },
      "source": [
        "STOP_TOKEN = '<stop>'\n",
        "PAD_TOKEN = '<pad>'\n",
        "\n",
        "\n",
        "CHUNK_SZ = 1000\n",
        "WINDOW_SZ = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8-DOLEAEdod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896076b3-6716-4f16-a07c-0b457de1f293"
      },
      "source": [
        "# Convert Text to array\n",
        "\n",
        "text = []\n",
        "root_dir = 'gdrive/Shared drives/CSCI1470 Final Project'\n",
        "with open(os.path.join(root_dir, 'songs.txt'), 'r') as data_file:\n",
        "  for line in data_file:\n",
        "    text.append(line.replace(\"d\", \" d\").split())\n",
        "test_song = text[0]\n",
        "text = text[1:]\n",
        "print(text[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['br', 'd316', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd4', '48', 'd1', 'br', 'd1', '46', 'd2', '47', 'd1', 'br', 'd1', '45', 'd1', 'br', 'd1', '43', 'd2', '44', 'd1', 'br', 'd1', '42', 'd1', 'br', 'd1', '40', 'd2', '41', 'd1', 'br', 'd15', '49', 'd1', 'br', 'd1', '47', 'd2', '48', 'd1', 'br', 'd1', '46', 'd1', 'br', 'd1', '44', 'd2', '45', 'd1', 'br', 'd1', '43', 'd1', 'br', 'd1', '41', 'd2', '42', 'd1', 'br', 'd1', '40', 'd1', 'br', 'd1', '38', 'd2', '39', 'd1', 'br', 'd1', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd29', '42', 'd29', '49', 'd25', '54', 'd37', 'br', 'd28', '31', 'd17', '43', 'd17', '55', 'd17', 'br', 'd16', '30', 'd13', '42', 'd13', '54', 'd5', 'br', 'd4', '49', 'd9', '55', 'd1', '56', 'd2', 'br', 'd1', '57', 'd1', '58', 'd1', '59', 'd2', 'br', 'd1', '60', 'd1', '61', 'd5', 'br', 'd6', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd4', '48', 'd1', 'br', 'd1', '46', 'd2', '47', 'd1', 'br', 'd1', '45', 'd1', 'br', 'd1', '43', 'd2', '44', 'd1', 'br', 'd1', '42', 'd1', 'br', 'd1', '40', 'd2', '41', 'd1', 'br', 'd23', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd29', '42', 'd29', '49', 'd25', '54', 'd37', 'br', 'd28', '31', 'd17', '43', 'd17', '55', 'd17', 'br', 'd16', '30', 'd17', '42', 'd17', '54', 'd11', 'br', 'd4', '49', 'd77', 'br', 'd5', '47', 'd2', '48', 'd1', 'br', 'd1', '46', 'd1', 'br', 'd1', '44', 'd2', '45', 'd1', 'br', 'd1', '43', 'd1', 'br', 'd1', '41', 'd2', 'br', 'd1', '40', 'd1', 'br', 'd1', '38', 'd2', '39', 'd1', 'br', 'd1', '37', 'd65', '44', 'd65', '53', 'd65', 'br', 'd64', '42', 'd33', '47', 'd65', '54', 'd29', '59', 'd29', 'br', 'd28', '37', 'd5', '44', 'd5', '49', 'd5', 'br', 'd4', '35', 'd33', '40', 'd33', '52', 'd33', 'br', 'd20', '42', 'd5', 'br', 'd8', '42', 'd5', 'br', 'd4', '37', 'd65', '44', 'd65', '49', 'd65', '53', 'd65', 'br', 'd64', '42', 'd33', '47', 'd57', '54', 'd29', '59', 'd29', 'br', 'd28', '37', 'd5', '44', 'd5', '49', 'd5', 'br', 'd4', '35', 'd33', '40', 'd29', '52', 'd25', 'br', 'd4', '38', 'd5', 'br', 'd20', '42', 'd5', '49', 'd5', '54', 'd9', 'br', 'd4', '47', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd4', '36', 'd2', '47', 'd2', '48', 'd1', 'br', 'd1', '35', 'd2', '46', 'd2', 'br', 'd1', '33', 'd2', '34', 'd1', '44', 'd2', '45', 'd1', 'br', 'd1', '32', 'd2', '43', 'd2', 'br', 'd1', '30', 'd2', '31', 'd1', '41', 'd2', '42', 'd1', 'br', 'd1', '29', 'd2', '40', 'd2', 'br', 'd23', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd29', '42', 'd29', '49', 'd25', '54', 'd49', 'br', 'd28', '31', 'd17', '43', 'd17', '55', 'd17', 'br', 'd16', '30', 'd13', '42', 'd13', 'br', 'd4', '49', 'd9', '55', 'd1', '56', 'd2', '57', 'd2', 'br', 'd1', '58', 'd1', '59', 'd2', '60', 'd2', 'br', 'd1', '61', 'd5', 'br', 'd6', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd4', '47', 'd2', '48', 'd1', 'br', 'd1', '46', 'd2', 'br', 'd1', '44', 'd2', '45', 'd1', 'br', 'd1', '43', 'd2', 'br', 'd1', '41', 'd2', '42', 'd1', 'br', 'd1', '40', 'd2', 'br', 'd23', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd29', '42', 'd29', '49', 'd25', '54', 'd37', 'br', 'd28', '31', 'd17', '43', 'd17', '55', 'd17', 'br', 'd16', '30', 'd17', '42', 'd14', '54', 'd7', 'br', 'd4', '49', 'd9', 'br', 'd2', '44', 'd1', '45', 'd2', '46', 'd2', 'br', 'd1', '47', 'd4', '48', 'd3', 'br', 'd3', '45', 'd2', '46', 'd1', 'br', 'd1', '44', 'd2', 'br', 'd1', '43', 'd1', 'br', 'd1', '41', 'd2', 'br', 'd1', '39', 'd2', '40', 'd1', 'br', 'd1', '38', 'd2', 'br', 'd1', '37', 'd53', '44', 'd51', '49', 'd51', '53', 'd51', 'br', 'd12', '35', 'd5', 'br', 'd16', '35', 'd5', 'br', 'd24', '28', 'd5', '40', 'd5', '47', 'd5', '52', 'd5', 'br', 'd4', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd15', '42', 'd18', '49', 'd18', '54', 'd18', 'br', 'd14', '31', 'd1', '33', 'd2', 'br', 'd1', '35', 'd2', 'br', 'd1', '37', 'd1', '38', 'd2', 'br', 'd1', '39', 'd2', 'br', 'd1', '40', 'd1', 'br', 'd6', '33', 'd5', '45', 'd5', '52', 'd5', '57', 'd5', 'br', 'd4', '34', 'd5', '46', 'd5', '53', 'd5', '58', 'd5', 'br', 'd4', '35', 'd5', '47', 'd5', '54', 'd5', '59', 'd5', 'br', 'd8', '35', 'd7', '47', 'd7', '54', 'd5', '59', 'd5', 'br', 'd8', '25', 'd5', '37', 'd5', '44', 'd5', '49', 'd5', 'br', 'd4', '28', 'd5', '35', 'd13', '40', 'd13', '47', 'd13', '52', 'd13', 'br', 'd4', '30', 'd5', '42', 'd5', 'br', 'd4', '28', 'd5', 'br', 'd4', '25', 'd5', '32', 'd5', '37', 'd57', '44', 'd55', '49', 'd55', 'br', 'd4', '53', 'd51', 'br', 'd12', '35', 'd5', 'br', 'd16', '35', 'd5', 'br', 'd24', '28', 'd5', '40', 'd5', '47', 'd5', '52', 'd5', 'br', 'd4', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd15', '42', 'd18', '49', 'd18', '54', 'd18', 'br', 'd14', '31', 'd1', '33', 'd2', 'br', 'd1', '35', 'd2', 'br', 'd1', '37', 'd1', '38', 'd2', 'br', 'd1', '39', 'd2', 'br', 'd1', '40', 'd1', 'br', 'd6', '33', 'd5', '45', 'd5', '52', 'd5', '57', 'd5', 'br', 'd4', '34', 'd5', '46', 'd5', '53', 'd5', '58', 'd5', 'br', 'd4', '35', 'd5', '47', 'd5', '54', 'd5', '59', 'd5', 'br', 'd8', '35', 'd7', '47', 'd7', '54', 'd5', '59', 'd5', 'br', 'd8', '25', 'd5', '37', 'd5', '44', 'd5', '49', 'd5', 'br', 'd4', '28', 'd5', '35', 'd13', '40', 'd13', '47', 'd13', '52', 'd13', 'br', 'd4', '30', 'd5', '42', 'd9', 'br', 'd8', '25', 'd5', '32', 'd5', '37', 'd57', '44', 'd55', '49', 'd55']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transposes music to generate more data\n",
        "transposed_songs = []\n",
        "for song in text:\n",
        "  for t in range(12):\n",
        "    transp = []\n",
        "    for note in song:\n",
        "      if note[0] != 'b' and note[0] != 'd':\n",
        "        transp.append(str(int(note) + t - 6))\n",
        "      else:\n",
        "        transp.append(note)\n",
        "    transposed_songs.append(transp)\n"
      ],
      "metadata": {
        "id": "sFhrLVCGh-zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(transposed_songs))\n",
        "print(str(int('1')+1))\n",
        "print(transposed_songs[0])\n",
        "print(transposed_songs[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRrmApCYkufJ",
        "outputId": "27cc0f42-8e0a-42a8-ecb9-eb43b381e12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29052\n",
            "2\n",
            "['br', 'd316', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd4', '48', 'd1', 'br', 'd1', '46', 'd2', '47', 'd1', 'br', 'd1', '45', 'd1', 'br', 'd1', '43', 'd2', '44', 'd1', 'br', 'd1', '42', 'd1', 'br', 'd1', '40', 'd2', '41', 'd1', 'br', 'd15', '49', 'd1', 'br', 'd1', '47', 'd2', '48', 'd1', 'br', 'd1', '46', 'd1', 'br', 'd1', '44', 'd2', '45', 'd1', 'br', 'd1', '43', 'd1', 'br', 'd1', '41', 'd2', '42', 'd1', 'br', 'd1', '40', 'd1', 'br', 'd1', '38', 'd2', '39', 'd1', 'br', 'd1', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd29', '42', 'd29', '49', 'd25', '54', 'd37', 'br', 'd28', '31', 'd17', '43', 'd17', '55', 'd17', 'br', 'd16', '30', 'd13', '42', 'd13', '54', 'd5', 'br', 'd4', '49', 'd9', '55', 'd1', '56', 'd2', 'br', 'd1', '57', 'd1', '58', 'd1', '59', 'd2', 'br', 'd1', '60', 'd1', '61', 'd5', 'br', 'd6', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd4', '48', 'd1', 'br', 'd1', '46', 'd2', '47', 'd1', 'br', 'd1', '45', 'd1', 'br', 'd1', '43', 'd2', '44', 'd1', 'br', 'd1', '42', 'd1', 'br', 'd1', '40', 'd2', '41', 'd1', 'br', 'd23', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd29', '42', 'd29', '49', 'd25', '54', 'd37', 'br', 'd28', '31', 'd17', '43', 'd17', '55', 'd17', 'br', 'd16', '30', 'd17', '42', 'd17', '54', 'd11', 'br', 'd4', '49', 'd77', 'br', 'd5', '47', 'd2', '48', 'd1', 'br', 'd1', '46', 'd1', 'br', 'd1', '44', 'd2', '45', 'd1', 'br', 'd1', '43', 'd1', 'br', 'd1', '41', 'd2', 'br', 'd1', '40', 'd1', 'br', 'd1', '38', 'd2', '39', 'd1', 'br', 'd1', '37', 'd65', '44', 'd65', '53', 'd65', 'br', 'd64', '42', 'd33', '47', 'd65', '54', 'd29', '59', 'd29', 'br', 'd28', '37', 'd5', '44', 'd5', '49', 'd5', 'br', 'd4', '35', 'd33', '40', 'd33', '52', 'd33', 'br', 'd20', '42', 'd5', 'br', 'd8', '42', 'd5', 'br', 'd4', '37', 'd65', '44', 'd65', '49', 'd65', '53', 'd65', 'br', 'd64', '42', 'd33', '47', 'd57', '54', 'd29', '59', 'd29', 'br', 'd28', '37', 'd5', '44', 'd5', '49', 'd5', 'br', 'd4', '35', 'd33', '40', 'd29', '52', 'd25', 'br', 'd4', '38', 'd5', 'br', 'd20', '42', 'd5', '49', 'd5', '54', 'd9', 'br', 'd4', '47', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd4', '36', 'd2', '47', 'd2', '48', 'd1', 'br', 'd1', '35', 'd2', '46', 'd2', 'br', 'd1', '33', 'd2', '34', 'd1', '44', 'd2', '45', 'd1', 'br', 'd1', '32', 'd2', '43', 'd2', 'br', 'd1', '30', 'd2', '31', 'd1', '41', 'd2', '42', 'd1', 'br', 'd1', '29', 'd2', '40', 'd2', 'br', 'd23', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd29', '42', 'd29', '49', 'd25', '54', 'd49', 'br', 'd28', '31', 'd17', '43', 'd17', '55', 'd17', 'br', 'd16', '30', 'd13', '42', 'd13', 'br', 'd4', '49', 'd9', '55', 'd1', '56', 'd2', '57', 'd2', 'br', 'd1', '58', 'd1', '59', 'd2', '60', 'd2', 'br', 'd1', '61', 'd5', 'br', 'd6', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd12', '35', 'd5', '47', 'd5', '54', 'd5', 'br', 'd4', '37', 'd5', '49', 'd5', '56', 'd5', 'br', 'd4', '47', 'd2', '48', 'd1', 'br', 'd1', '46', 'd2', 'br', 'd1', '44', 'd2', '45', 'd1', 'br', 'd1', '43', 'd2', 'br', 'd1', '41', 'd2', '42', 'd1', 'br', 'd1', '40', 'd2', 'br', 'd23', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd29', '42', 'd29', '49', 'd25', '54', 'd37', 'br', 'd28', '31', 'd17', '43', 'd17', '55', 'd17', 'br', 'd16', '30', 'd17', '42', 'd14', '54', 'd7', 'br', 'd4', '49', 'd9', 'br', 'd2', '44', 'd1', '45', 'd2', '46', 'd2', 'br', 'd1', '47', 'd4', '48', 'd3', 'br', 'd3', '45', 'd2', '46', 'd1', 'br', 'd1', '44', 'd2', 'br', 'd1', '43', 'd1', 'br', 'd1', '41', 'd2', 'br', 'd1', '39', 'd2', '40', 'd1', 'br', 'd1', '38', 'd2', 'br', 'd1', '37', 'd53', '44', 'd51', '49', 'd51', '53', 'd51', 'br', 'd12', '35', 'd5', 'br', 'd16', '35', 'd5', 'br', 'd24', '28', 'd5', '40', 'd5', '47', 'd5', '52', 'd5', 'br', 'd4', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd15', '42', 'd18', '49', 'd18', '54', 'd18', 'br', 'd14', '31', 'd1', '33', 'd2', 'br', 'd1', '35', 'd2', 'br', 'd1', '37', 'd1', '38', 'd2', 'br', 'd1', '39', 'd2', 'br', 'd1', '40', 'd1', 'br', 'd6', '33', 'd5', '45', 'd5', '52', 'd5', '57', 'd5', 'br', 'd4', '34', 'd5', '46', 'd5', '53', 'd5', '58', 'd5', 'br', 'd4', '35', 'd5', '47', 'd5', '54', 'd5', '59', 'd5', 'br', 'd8', '35', 'd7', '47', 'd7', '54', 'd5', '59', 'd5', 'br', 'd8', '25', 'd5', '37', 'd5', '44', 'd5', '49', 'd5', 'br', 'd4', '28', 'd5', '35', 'd13', '40', 'd13', '47', 'd13', '52', 'd13', 'br', 'd4', '30', 'd5', '42', 'd5', 'br', 'd4', '28', 'd5', 'br', 'd4', '25', 'd5', '32', 'd5', '37', 'd57', '44', 'd55', '49', 'd55', 'br', 'd4', '53', 'd51', 'br', 'd12', '35', 'd5', 'br', 'd16', '35', 'd5', 'br', 'd24', '28', 'd5', '40', 'd5', '47', 'd5', '52', 'd5', 'br', 'd4', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd15', '42', 'd18', '49', 'd18', '54', 'd18', 'br', 'd14', '31', 'd1', '33', 'd2', 'br', 'd1', '35', 'd2', 'br', 'd1', '37', 'd1', '38', 'd2', 'br', 'd1', '39', 'd2', 'br', 'd1', '40', 'd1', 'br', 'd6', '33', 'd5', '45', 'd5', '52', 'd5', '57', 'd5', 'br', 'd4', '34', 'd5', '46', 'd5', '53', 'd5', '58', 'd5', 'br', 'd4', '35', 'd5', '47', 'd5', '54', 'd5', '59', 'd5', 'br', 'd8', '35', 'd7', '47', 'd7', '54', 'd5', '59', 'd5', 'br', 'd8', '25', 'd5', '37', 'd5', '44', 'd5', '49', 'd5', 'br', 'd4', '28', 'd5', '35', 'd13', '40', 'd13', '47', 'd13', '52', 'd13', 'br', 'd4', '30', 'd5', '42', 'd9', 'br', 'd8', '25', 'd5', '32', 'd5', '37', 'd57', '44', 'd55', '49', 'd55']\n",
            "['br', 'd316', '36', 'd5', '48', 'd5', '55', 'd5', 'br', 'd4', '38', 'd5', '50', 'd5', '57', 'd5', 'br', 'd12', '36', 'd5', '48', 'd5', '55', 'd5', 'br', 'd4', '38', 'd5', '50', 'd5', '57', 'd5', 'br', 'd12', '36', 'd5', '48', 'd5', '55', 'd5', 'br', 'd4', '38', 'd5', '50', 'd5', '57', 'd5', 'br', 'd4', '49', 'd1', 'br', 'd1', '47', 'd2', '48', 'd1', 'br', 'd1', '46', 'd1', 'br', 'd1', '44', 'd2', '45', 'd1', 'br', 'd1', '43', 'd1', 'br', 'd1', '41', 'd2', '42', 'd1', 'br', 'd15', '50', 'd1', 'br', 'd1', '48', 'd2', '49', 'd1', 'br', 'd1', '47', 'd1', 'br', 'd1', '45', 'd2', '46', 'd1', 'br', 'd1', '44', 'd1', 'br', 'd1', '42', 'd2', '43', 'd1', 'br', 'd1', '41', 'd1', 'br', 'd1', '39', 'd2', '40', 'd1', 'br', 'd1', '30', 'd5', '42', 'd5', '49', 'd5', '54', 'd5', 'br', 'd4', '31', 'd29', '43', 'd29', '50', 'd25', '55', 'd37', 'br', 'd28', '32', 'd17', '44', 'd17', '56', 'd17', 'br', 'd16', '31', 'd13', '43', 'd13', '55', 'd5', 'br', 'd4', '50', 'd9', '56', 'd1', '57', 'd2', 'br', 'd1', '58', 'd1', '59', 'd1', '60', 'd2', 'br', 'd1', '61', 'd1', '62', 'd5', 'br', 'd6', '36', 'd5', '48', 'd5', '55', 'd5', 'br', 'd4', '38', 'd5', '50', 'd5', '57', 'd5', 'br', 'd12', '36', 'd5', '48', 'd5', '55', 'd5', 'br', 'd4', '38', 'd5', '50', 'd5', '57', 'd5', 'br', 'd12', '36', 'd5', '48', 'd5', '55', 'd5', 'br', 'd4', '38', 'd5', '50', 'd5', '57', 'd5', 'br', 'd4', '49', 'd1', 'br', 'd1', '47', 'd2', '48', 'd1', 'br', 'd1', '46', 'd1', 'br', 'd1', '44', 'd2', '45', 'd1', 'br', 'd1', '43', 'd1', 'br', 'd1', '41', 'd2', '42', 'd1', 'br', 'd23', '30', 'd5', '42', 'd5', '49', 'd5', '54', 'd5', 'br', 'd4', '31', 'd29', '43', 'd29', '50', 'd25', '55', 'd37', 'br', 'd28', '32', 'd17', '44', 'd17', '56', 'd17', 'br', 'd16', '31', 'd17', '43', 'd17', '55', 'd11', 'br', 'd4', '50', 'd77', 'br', 'd5', '48', 'd2', '49', 'd1', 'br', 'd1', '47', 'd1', 'br', 'd1', '45', 'd2', '46', 'd1', 'br', 'd1', '44', 'd1', 'br', 'd1', '42', 'd2', 'br', 'd1', '41', 'd1', 'br', 'd1', '39', 'd2', '40', 'd1', 'br', 'd1', '38', 'd65', '45', 'd65', '54', 'd65', 'br', 'd64', '43', 'd33', '48', 'd65', '55', 'd29', '60', 'd29', 'br', 'd28', '38', 'd5', '45', 'd5', '50', 'd5', 'br', 'd4', '36', 'd33', '41', 'd33', '53', 'd33', 'br', 'd20', '43', 'd5', 'br', 'd8', '43', 'd5', 'br', 'd4', '38', 'd65', '45', 'd65', '50', 'd65', '54', 'd65', 'br', 'd64', '43', 'd33', '48', 'd57', '55', 'd29', '60', 'd29', 'br', 'd28', '38', 'd5', '45', 'd5', '50', 'd5', 'br', 'd4', '36', 'd33', '41', 'd29', '53', 'd25', 'br', 'd4', '39', 'd5', 'br', 'd20', '43', 'd5', '50', 'd5', '55', 'd9', 'br', 'd4', '48', 'd5', 'br', 'd4', '38', 'd5', '50', 'd5', '57', 'd5', 'br', 'd12', '36', 'd5', '48', 'd5', '55', 'd5', 'br', 'd4', '38', 'd5', '50', 'd5', '57', 'd5', 'br', 'd12', '36', 'd5', '48', 'd5', '55', 'd5', 'br', 'd4', '38', 'd5', '50', 'd5', '57', 'd5', 'br', 'd4', '37', 'd2', '48', 'd2', '49', 'd1', 'br', 'd1', '36', 'd2', '47', 'd2', 'br', 'd1', '34', 'd2', '35', 'd1', '45', 'd2', '46', 'd1', 'br', 'd1', '33', 'd2', '44', 'd2', 'br', 'd1', '31', 'd2', '32', 'd1', '42', 'd2', '43', 'd1', 'br', 'd1', '30', 'd2', '41', 'd2', 'br', 'd23', '30', 'd5', '42', 'd5', '49', 'd5', '54', 'd5', 'br', 'd4', '31', 'd29', '43', 'd29', '50', 'd25', '55', 'd49', 'br', 'd28', '32', 'd17', '44', 'd17', '56', 'd17', 'br', 'd16', '31', 'd13', '43', 'd13', 'br', 'd4', '50', 'd9', '56', 'd1', '57', 'd2', '58', 'd2', 'br', 'd1', '59', 'd1', '60', 'd2', '61', 'd2', 'br', 'd1', '62', 'd5', 'br', 'd6', '36', 'd5', '48', 'd5', '55', 'd5', 'br', 'd4', '38', 'd5', '50', 'd5', '57', 'd5', 'br', 'd12', '36', 'd5', '48', 'd5', '55', 'd5', 'br', 'd4', '38', 'd5', '50', 'd5', '57', 'd5', 'br', 'd12', '36', 'd5', '48', 'd5', '55', 'd5', 'br', 'd4', '38', 'd5', '50', 'd5', '57', 'd5', 'br', 'd4', '48', 'd2', '49', 'd1', 'br', 'd1', '47', 'd2', 'br', 'd1', '45', 'd2', '46', 'd1', 'br', 'd1', '44', 'd2', 'br', 'd1', '42', 'd2', '43', 'd1', 'br', 'd1', '41', 'd2', 'br', 'd23', '30', 'd5', '42', 'd5', '49', 'd5', '54', 'd5', 'br', 'd4', '31', 'd29', '43', 'd29', '50', 'd25', '55', 'd37', 'br', 'd28', '32', 'd17', '44', 'd17', '56', 'd17', 'br', 'd16', '31', 'd17', '43', 'd14', '55', 'd7', 'br', 'd4', '50', 'd9', 'br', 'd2', '45', 'd1', '46', 'd2', '47', 'd2', 'br', 'd1', '48', 'd4', '49', 'd3', 'br', 'd3', '46', 'd2', '47', 'd1', 'br', 'd1', '45', 'd2', 'br', 'd1', '44', 'd1', 'br', 'd1', '42', 'd2', 'br', 'd1', '40', 'd2', '41', 'd1', 'br', 'd1', '39', 'd2', 'br', 'd1', '38', 'd53', '45', 'd51', '50', 'd51', '54', 'd51', 'br', 'd12', '36', 'd5', 'br', 'd16', '36', 'd5', 'br', 'd24', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd5', '42', 'd5', '49', 'd5', '54', 'd5', 'br', 'd4', '31', 'd15', '43', 'd18', '50', 'd18', '55', 'd18', 'br', 'd14', '32', 'd1', '34', 'd2', 'br', 'd1', '36', 'd2', 'br', 'd1', '38', 'd1', '39', 'd2', 'br', 'd1', '40', 'd2', 'br', 'd1', '41', 'd1', 'br', 'd6', '34', 'd5', '46', 'd5', '53', 'd5', '58', 'd5', 'br', 'd4', '35', 'd5', '47', 'd5', '54', 'd5', '59', 'd5', 'br', 'd4', '36', 'd5', '48', 'd5', '55', 'd5', '60', 'd5', 'br', 'd8', '36', 'd7', '48', 'd7', '55', 'd5', '60', 'd5', 'br', 'd8', '26', 'd5', '38', 'd5', '45', 'd5', '50', 'd5', 'br', 'd4', '29', 'd5', '36', 'd13', '41', 'd13', '48', 'd13', '53', 'd13', 'br', 'd4', '31', 'd5', '43', 'd5', 'br', 'd4', '29', 'd5', 'br', 'd4', '26', 'd5', '33', 'd5', '38', 'd57', '45', 'd55', '50', 'd55', 'br', 'd4', '54', 'd51', 'br', 'd12', '36', 'd5', 'br', 'd16', '36', 'd5', 'br', 'd24', '29', 'd5', '41', 'd5', '48', 'd5', '53', 'd5', 'br', 'd4', '30', 'd5', '42', 'd5', '49', 'd5', '54', 'd5', 'br', 'd4', '31', 'd15', '43', 'd18', '50', 'd18', '55', 'd18', 'br', 'd14', '32', 'd1', '34', 'd2', 'br', 'd1', '36', 'd2', 'br', 'd1', '38', 'd1', '39', 'd2', 'br', 'd1', '40', 'd2', 'br', 'd1', '41', 'd1', 'br', 'd6', '34', 'd5', '46', 'd5', '53', 'd5', '58', 'd5', 'br', 'd4', '35', 'd5', '47', 'd5', '54', 'd5', '59', 'd5', 'br', 'd4', '36', 'd5', '48', 'd5', '55', 'd5', '60', 'd5', 'br', 'd8', '36', 'd7', '48', 'd7', '55', 'd5', '60', 'd5', 'br', 'd8', '26', 'd5', '38', 'd5', '45', 'd5', '50', 'd5', 'br', 'd4', '29', 'd5', '36', 'd13', '41', 'd13', '48', 'd13', '53', 'd13', 'br', 'd4', '31', 'd5', '43', 'd9', 'br', 'd8', '26', 'd5', '33', 'd5', '38', 'd57', '45', 'd55', '50', 'd55']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqul2N0zLWWj"
      },
      "source": [
        "# Pad songs\n",
        "def pad_songs(songs):\n",
        "  padded_songs = []\n",
        "  for line in songs:\n",
        "    if len(line) != 0:\n",
        "      song = line[:CHUNK_SZ]\n",
        "      song += [PAD_TOKEN] * (CHUNK_SZ - len(song))\n",
        "      padded_songs.append(song)\n",
        "  return padded_songs\n",
        "\n",
        "orig = pad_songs(text)\n",
        "transposed = pad_songs(transposed_songs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orRh13byKfIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25d0c15-6381-4650-98c8-87c776fca392"
      },
      "source": [
        "# Create vocab\n",
        "\n",
        "tokens = []\n",
        "for s in transposed: tokens.extend(s)\n",
        "all_words = sorted(list(set([STOP_TOKEN,PAD_TOKEN] + tokens)))\n",
        "\n",
        "vocab =  {word:i for i,word in enumerate(all_words)}\n",
        "VOCAB_SZ = len(vocab)\n",
        "print(VOCAB_SZ)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAGJT3AQOriU"
      },
      "source": [
        "orig_ids = np.asarray(np.stack([[vocab[word] for word in song] for song in orig]))\n",
        "transposed_ids = np.asarray(np.stack([[vocab[word] for word in song] for song in transposed]))\n",
        "test_ids = np.asarray(np.stack([vocab[word] for word in test_song]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(orig_ids)\n",
        "print(transposed_ids)"
      ],
      "metadata": {
        "id": "dY_JTxX-pOBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jGYG5x6golj"
      },
      "source": [
        "# Transformer model\n",
        "def Attention_Matrix(K, Q, use_mask=False):\n",
        "\t\"\"\"\n",
        "\tThis functions runs a single attention head.\n",
        "\t:param K: is [batch_size x window_size_keys x embedding_size]\n",
        "\t:param Q: is [batch_size x window_size_queries x embedding_size]\n",
        "\t:return: attention matrix\n",
        "\t\"\"\"\n",
        "\t\n",
        "\twindow_size_queries = Q.get_shape()[1] # window size of queries\n",
        "\twindow_size_keys = K.get_shape()[1] # window size of keys\n",
        "\troot_dk = window_size_keys ** 0.5\n",
        "\tmask = tf.convert_to_tensor(value=np.transpose(np.tril(np.ones((window_size_queries,window_size_keys))*np.NINF,-1),(1,0)),dtype=tf.float32)\n",
        "\tatten_mask = tf.tile(tf.reshape(mask,[-1,window_size_queries,window_size_keys]),[tf.shape(input=K)[0],1,1])\n",
        "\n",
        "\tunmasked = tf.math.scalar_mul(root_dk, tf.matmul(Q, K, transpose_b=True))\n",
        "\n",
        "\tmasked = tf.add(unmasked, atten_mask) if use_mask else unmasked\n",
        "\treturn tf.nn.softmax(masked)\n",
        "\n",
        "\n",
        "class Atten_Head(tf.keras.layers.Layer):\n",
        "\tdef __init__(self, input_size, output_size, use_mask):\t\t\n",
        "\t\tsuper(Atten_Head, self).__init__()\n",
        "\n",
        "\t\tself.use_mask = use_mask\n",
        "\n",
        "\t\tself.WK = self.add_weight(shape=(input_size, output_size), dtype=tf.float32, trainable=True)\n",
        "\t\tself.WQ = self.add_weight(shape=(input_size, output_size), dtype=tf.float32, trainable=True)\n",
        "\t\tself.WV = self.add_weight(shape=(input_size, output_size), dtype=tf.float32, trainable=True)\n",
        "\t\t\n",
        "\t@tf.function\n",
        "\tdef call(self, inputs_for_keys, inputs_for_values, inputs_for_queries):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\t:param inputs_for_keys: tensor of [batch_size x [ENG/FRN]_WINDOW_SIZE x input_size ]\n",
        "\t\t:param inputs_for_values: tensor of [batch_size x [ENG/FRN]_WINDOW_SIZE x input_size ]\n",
        "\t\t:param inputs_for_queries: tensor of [batch_size x [ENG/FRN]_WINDOW_SIZE x input_size ]\n",
        "\t\t:return: tensor of [BATCH_SIZE x (ENG/FRN)_WINDOW_SIZE x output_size ]\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tK = tf.tensordot(inputs_for_keys, self.WK, axes=[[2], [1]])\n",
        "\t\tV = tf.tensordot(inputs_for_keys, self.WV, axes=[[2], [1]])\n",
        "\t\tQ = tf.tensordot(inputs_for_keys, self.WQ, axes=[[2], [1]])\n",
        "\n",
        "\t\treturn tf.matmul(Attention_Matrix(K, Q, self.use_mask), V)\n",
        "\n",
        "class Feed_Forwards(tf.keras.layers.Layer):\n",
        "\tdef __init__(self, emb_sz):\n",
        "\t\tsuper(Feed_Forwards, self).__init__()\n",
        "\n",
        "\t\tself.layer_1 = tf.keras.layers.Dense(emb_sz,activation='relu')\n",
        "\t\tself.layer_2 = tf.keras.layers.Dense(emb_sz)\n",
        "\n",
        "\t@tf.function\n",
        "\tdef call(self, inputs):\n",
        "\t\tlayer_1_out = self.layer_1(inputs)\n",
        "\t\tlayer_2_out = self.layer_2(layer_1_out)\n",
        "\t\treturn layer_2_out\n",
        "\n",
        "class Transformer_Block(tf.keras.layers.Layer):\n",
        "\tdef __init__(self, emb_sz, is_decoder):\n",
        "\t\tsuper(Transformer_Block, self).__init__()\n",
        "\n",
        "\t\tself.ff_layer = Feed_Forwards(emb_sz)\n",
        "\t\tself.self_atten = Atten_Head(emb_sz,emb_sz,use_mask=is_decoder)\n",
        "\t\tself.is_decoder = is_decoder\n",
        "\t\tif self.is_decoder:\n",
        "\t\t\tself.self_context_atten = Atten_Head(emb_sz,emb_sz,use_mask=False)\n",
        "\n",
        "\t\tself.layer_norm = tf.keras.layers.LayerNormalization(axis=-1)\n",
        "\n",
        "\t@tf.function\n",
        "\tdef call(self, inputs, context=None):\n",
        "\n",
        "\t\tatten_out = self.self_atten(inputs,inputs,inputs)\n",
        "\t\tatten_out+=inputs\n",
        "\t\tatten_normalized = self.layer_norm(atten_out)\n",
        "\n",
        "\t\tif self.is_decoder:\n",
        "\t\t\tassert context is not None,\"Decoder blocks require context\"\n",
        "\t\t\tcontext_atten_out = self.self_context_atten(context,context,atten_normalized)\n",
        "\t\t\tcontext_atten_out+=atten_normalized\n",
        "\t\t\tatten_normalized = self.layer_norm(context_atten_out)\n",
        "\n",
        "\t\tff_out=self.ff_layer(atten_normalized)\n",
        "\t\tff_out+=atten_normalized\n",
        "\t\tff_norm = self.layer_norm(ff_out)\n",
        "\n",
        "\t\treturn tf.nn.relu(ff_norm)\n",
        "\n",
        "class Position_Encoding_Layer(tf.keras.layers.Layer):\n",
        "\tdef __init__(self, window_sz, emb_sz):\n",
        "\t\tsuper(Position_Encoding_Layer, self).__init__()\n",
        "\t\tself.positional_embeddings = self.add_weight(\"pos_embed\",shape=[window_sz, emb_sz])\n",
        "\n",
        "\t@tf.function\n",
        "\tdef call(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tAdds positional embeddings to word embeddings.    \n",
        "\t\t:param x: [BATCH_SIZE x (ENG/FRN)_WINDOW_SIZE x EMBEDDING_SIZE ] the input embeddings fed to the encoder\n",
        "\t\t:return: [BATCH_SIZE x (ENG/FRN)_WINDOW_SIZE x EMBEDDING_SIZE ] new word embeddings with added positional encodings\n",
        "\t\t\"\"\"\n",
        "\t\treturn x+self.positional_embeddings\n",
        "\n",
        "class Transformer_Seq2Seq(tf.keras.Model):\n",
        "\tdef __init__(self, window_sz, vocab_sz):\n",
        "\t\tsuper(Transformer_Seq2Seq, self).__init__()\n",
        "\n",
        "\t\tself.vocab_sz = vocab_sz\n",
        "\t\tself.window_sz = window_sz \n",
        "\n",
        "\t\t# Define batch size and optimizer/learning rate\n",
        "\t\tself.batch_size = 50\n",
        "\t\tself.embedding_size = 128\n",
        "\t\tself.learning_rate = 0.0015\n",
        "\t\tself.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "\t\t# Define english and french embedding layers:\n",
        "\t\tself.embedding = tf.keras.layers.Embedding(self.vocab_sz, self.embedding_size)\n",
        "\t\t# Create positional encoder layers\n",
        "\t\tself.position = Position_Encoding_Layer(self.window_sz, self.embedding_size)\n",
        "\t\t# Define encoder and decoder layers:\n",
        "\t\tself.encoder = Transformer_Block(self.embedding_size, False)\n",
        "\t\tself.decoder = Transformer_Block(self.embedding_size, True)\n",
        "\t\t# Define dense layer(s)\n",
        "\t\tself.dense1 = tf.keras.layers.Dense(5, activation='relu')\n",
        "\t\tself.dense2 = tf.keras.layers.Dense(self.vocab_sz, activation='softmax')\n",
        "\n",
        "\t@tf.function\n",
        "\tdef call(self, encoder_input, decoder_input):\n",
        "\t\t\"\"\"\n",
        "\t\t:param encoder_input: batched ids corresponding to french sentences\n",
        "\t\t:param decoder_input: batched ids corresponding to english sentences\n",
        "\t\t:return prbs: The 3d probabilities as a tensor, [batch_size x window_size x english_vocab_size]\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t#1) Add the positional embeddings to french sentence embeddings\n",
        "\t\tlocated_given = self.position(self.embedding(encoder_input))\n",
        "\t\t#2) Pass the french sentence embeddings to the encoder\n",
        "\t\tencoded = self.encoder(located_given)\n",
        "\t\t#3) Add positional embeddings to the english sentence embeddings\n",
        "\t\tlocated_wanted = self.position(self.embedding(decoder_input))\n",
        "\t\t#4) Pass the english embeddings and output of your encoder, to the decoder\n",
        "\t\tdecoded = self.decoder(located_wanted, encoded)\n",
        "\t\t#5) Apply dense layer(s) to the decoder out to generate probabilities\n",
        "\t\tprobabilities = self.dense2(self.dense1(decoded))\n",
        "\t\treturn probabilities\n",
        "\n",
        "\tdef loss(self, probs, labels):\n",
        "\t\t\t\t\treturn tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n",
        "\t\t\n",
        "def train(model, inputs):\n",
        "\t\"\"\"\n",
        "\tInputs: list of sequence of notes as tokens\n",
        "\t\"\"\"\n",
        "\tunshuffled_inputs = []\n",
        "\tsequed = []\n",
        "\tdef seq_window(sequences, window_size):\n",
        "\t\tdata = []\n",
        "\t\tfor ind in range(len(sequences)):\n",
        "\t\t\tseq_len = len(sequences[ind])\n",
        "\t\t\tif seq_len < window_size:\n",
        "\t\t\t\traise Exception(\"Window too large for sequence\")\n",
        "\t\t\twindows = []\n",
        "\t\t\tfor window in range(seq_len - window_size + 1):\n",
        "\t\t\t\twindows.append((ind, window))\n",
        "\t\t\tdata += windows\n",
        "\t\treturn data\n",
        "\t\n",
        "\tdef to_window(bat, inputs, is_label):\n",
        "\t\tnew_windows = np.zeros([model.batch_size, WINDOW_SZ])\n",
        "\t\tfor i in range(len(bat)):\n",
        "\t\t\tinds = bat[i]\n",
        "\t\t\tseq = inputs[inds[0]]\n",
        "\t\t\t\n",
        "\t\t\tif is_label:\n",
        "\t\t\t\tnew_windows[i] = seq[inds[1]+1:inds[1]+1+WINDOW_SZ]\n",
        "\t\t\telse:\n",
        "\t\t\t\tnew_windows[i] = seq[inds[1]:inds[1]+WINDOW_SZ]\n",
        "\t\treturn new_windows\n",
        "\n",
        "\tunshuffled_inputs = seq_window(inputs[:, :-1], WINDOW_SZ)\n",
        "\tprint(unshuffled_inputs[0])\n",
        "\tindices = np.arange(len(unshuffled_inputs))\n",
        "\tnp.random.shuffle(indices)\n",
        "\n",
        "\tsequed = np.take(np.asarray(unshuffled_inputs), indices, axis=0)\n",
        "\tprint(sequed[0])\n",
        "\tlass = 0\n",
        "\tlosses = []\n",
        "\tfor batch in range(math.floor(len(inputs) / model.batch_size)):\n",
        "\t\tbatch_1 = sequed[batch * model.batch_size:batch * model.batch_size + model.batch_size]\n",
        "\t\tbatch_en_in = to_window(batch_1, inputs, False)\n",
        "\t\tbatch_de_la = to_window(batch_1, inputs, True)\n",
        "\n",
        "\t\tif batch%20 == 0:\n",
        "\t\t\tprint(lass / 20)\n",
        "\t\t\tlass = 0\n",
        "\t\telse:\n",
        "\t\t\tpass\t\n",
        "\t\twith tf.GradientTape() as tape:\n",
        "\t\t\tpredictions = model.call(batch_en_in, batch_en_in)\n",
        "\t\t\tloss = model.loss(predictions, batch_de_la)\n",
        "\t\t\tlass += loss\n",
        "\t\t\tlosses.append(loss)\n",
        "\t\t\tgradients = tape.gradient(loss, model.trainable_variables)\n",
        "\t\t\tmodel.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\treturn losses\n",
        "\n",
        "def generate_sentence(sample, length, vocab, model, sample_n=10):\n",
        "\treverse_vocab = {idx: word for word, idx in vocab.items()}\n",
        "\tnotes = [sample]\n",
        "\tnext_input = np.asarray([sample])\n",
        "\tsong = np.asarray([sample])\n",
        "\n",
        "\tfor i in range(length):\n",
        "\t\tlogits = model.call(next_input, next_input)\n",
        "\t\tlogits = np.array(logits[0, WINDOW_SZ - 1,:])\n",
        "\t\ttop_n = np.argsort(logits)[-sample_n:]\t\t\t\n",
        "\t\tn_logits = np.exp(logits[top_n])/np.exp(logits[top_n]).sum()\n",
        "\t\tout_index = np.random.choice(top_n,p=n_logits)\n",
        "\t\tsong = np.append(song, [[out_index]], axis=1)\n",
        "\t\tnext_input = np.append(next_input[:, 1:], [[out_index]], axis=1)\n",
        "\n",
        "\treturn song\n",
        "\n",
        "model = Transformer_Seq2Seq(WINDOW_SZ, VOCAB_SZ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZHgD_rPEMoN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3001ac33-5782-4da2-9c07-fefcdffe848e"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "losses = []\n",
        "for i in range(1):\n",
        "  losses += train(model, transposed_ids)\n",
        "for i in range(3):\n",
        " losses += train(model, orig_ids)\n",
        "plt.plot()\n",
        "plt.show(losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0)\n",
            "[6198   51]\n",
            "0.0\n",
            "tf.Tensor(6.0327487, shape=(), dtype=float32)\n",
            "tf.Tensor(4.7893343, shape=(), dtype=float32)\n",
            "tf.Tensor(4.0625277, shape=(), dtype=float32)\n",
            "tf.Tensor(3.7544446, shape=(), dtype=float32)\n",
            "tf.Tensor(3.5167038, shape=(), dtype=float32)\n",
            "tf.Tensor(3.3086681, shape=(), dtype=float32)\n",
            "tf.Tensor(3.210307, shape=(), dtype=float32)\n",
            "tf.Tensor(3.1542475, shape=(), dtype=float32)\n",
            "tf.Tensor(3.089437, shape=(), dtype=float32)\n",
            "tf.Tensor(3.0790467, shape=(), dtype=float32)\n",
            "tf.Tensor(3.0321348, shape=(), dtype=float32)\n",
            "tf.Tensor(3.03453, shape=(), dtype=float32)\n",
            "tf.Tensor(3.0127358, shape=(), dtype=float32)\n",
            "tf.Tensor(3.017417, shape=(), dtype=float32)\n",
            "tf.Tensor(3.0069475, shape=(), dtype=float32)\n",
            "tf.Tensor(3.0047174, shape=(), dtype=float32)\n",
            "tf.Tensor(3.0039973, shape=(), dtype=float32)\n",
            "tf.Tensor(3.0064929, shape=(), dtype=float32)\n",
            "tf.Tensor(3.0222945, shape=(), dtype=float32)\n",
            "tf.Tensor(3.0217137, shape=(), dtype=float32)\n",
            "tf.Tensor(2.9608958, shape=(), dtype=float32)\n",
            "tf.Tensor(2.9752595, shape=(), dtype=float32)\n",
            "tf.Tensor(2.9822285, shape=(), dtype=float32)\n",
            "tf.Tensor(3.0039723, shape=(), dtype=float32)\n",
            "tf.Tensor(2.9700067, shape=(), dtype=float32)\n",
            "tf.Tensor(2.944831, shape=(), dtype=float32)\n",
            "tf.Tensor(2.9523456, shape=(), dtype=float32)\n",
            "tf.Tensor(2.954484, shape=(), dtype=float32)\n",
            "tf.Tensor(2.9688618, shape=(), dtype=float32)\n",
            "(0, 0)\n",
            "[1348  272]\n",
            "0.0\n",
            "tf.Tensor(2.9412339, shape=(), dtype=float32)\n",
            "tf.Tensor(2.9282959, shape=(), dtype=float32)\n",
            "(0, 0)\n",
            "[671 783]\n",
            "0.0\n",
            "tf.Tensor(2.8966768, shape=(), dtype=float32)\n",
            "tf.Tensor(2.9168277, shape=(), dtype=float32)\n",
            "(0, 0)\n",
            "[306 269]\n",
            "0.0\n",
            "tf.Tensor(2.920939, shape=(), dtype=float32)\n",
            "tf.Tensor(2.9148822, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = orig_ids[1986, 30:130]\n",
        "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
        "string = \"\"\n",
        "for id in generate_sentence(sample, 400, vocab, model).tolist()[0]:\n",
        "  string += str(reverse_vocab[id]) + \" \"\n",
        "\n",
        "print(string)"
      ],
      "metadata": {
        "id": "x5z6GDBsJtHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10488fd5-ef0b-4b99-e92f-171b2d23052e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47 d5 br d3 57 d6 br d1 45 d5 br d3 56 d5 br d1 44 d5 br d3 54 d3 br d1 42 d6 57 d63 61 d127 66 d63 br d4 54 d2 br d4 37 d6 54 d3 br d2 53 d3 br d2 54 d2 br d2 56 d3 br d2 42 d5 br d6 37 d7 br d4 56 d3 br d2 40 d5 br d2 59 d3 br d2 42 d7 br d8 37 d6 br d8 42 d7 62 d4 br d4 60 d3 br d2 37 d7 br d4 56 d3 60 d11 55 d7 br d6 67 d7 62 d1 69 d3 62 d9 br d2 52 d6 55 d6 br d2 59 d4 62 d10 60 d2 59 d3 64 d4 33 d2 57 d3 55 d2 59 d10 57 d1 55 d2 55 d3 60 d11 55 d5 69 d2 59 d9 60 d10 57 d7 62 d7 69 d3 62 d4 38 d9 60 d4 67 d11 59 d1 36 d4 52 d17 69 d4 64 d1 57 d7 67 d3 64 d4 33 d8 62 d3 br d7 57 d9 60 d6 67 d9 64 d9 br d16 67 d1 57 d2 60 d5 67 d5 69 d9 br d3 br d2 60 d2 52 d1 59 d10 64 d10 67 d1 64 d8 57 d4 62 d11 br d3 67 d3 br d2 62 d8 31 d3 57 d17 57 d5 57 d9 59 d13 59 d3 br d10 57 d9 br d5 62 d4 64 d13 59 d17 60 d5 69 d2 55 d5 55 d3 59 d9 60 d7 71 d7 59 d2 55 d4 52 d6 67 d10 55 d3 64 d2 57 d5 69 d2 57 d6 br d3 55 d1 52 d1 52 d3 64 d11 br d12 52 d8 59 d3 71 d4 33 d17 57 d7 55 d1 br d2 br d6 60 d9 69 d6 br d7 57 d2 60 d1 36 d1 64 d1 31 d4 62 d7 59 d3 64 d2 69 d5 br d16 59 d17 55 d1 64 d13 64 d5 55 d3 59 d6 60 d2 69 d13 69 d4 59 d8 57 d7 69 d7 64 d17 71 d8 br d4 33 d2 br d8 31 d7 67 d9 55 d10 64 d33 60 d3 64 d9 64 d65 57 d13 59 d13 55 d1 55 d17 57 d7 55 d10 71 d2 64 d3 60 d33 60 d5 71 d2 59 d9 71 d3 60 d7 64 d2 55 d13 55 d13 60 d2 55 d5 br d2 69 d33 64 d6 br d3 br d12 57 d17 57 d4 55 d5 69 d6 br d8 31 d4 36 d9 br d6 71 d6 69 d3 55 d4 38 d11 62 d5 br d4 36 d1 62 d5 67 d5 71 d2 52 d4 38 d9 59 d13 59 d9 57 d33 64 d7 55 d5 62 d2 64 d4 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
        "for id in song_ids[0, :20]: print(reverse_vocab[id]) "
      ],
      "metadata": {
        "id": "M0RSltFBanhk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "bed2c923-a6dc-4b35-a400-ccfb9410bb39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-da50cb9e4f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreverse_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msong_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreverse_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'song_ids' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes in a string of tokens representing one song, transforms into a pianoroll matrix\n",
        "def token_to_music(tokens):\n",
        "  token_list = tokens.replace(\" d\", \"d\").split()\n",
        "  mat = np.full([1, 128], False)\n",
        "  note = 0\n",
        "  for tk in token_list:\n",
        "    pitch_dur = tk.split('d')\n",
        "    if len(pitch_dur) == 2:\n",
        "      pitch = tk.split('d')[0]\n",
        "      dur = int(tk.split('d')[1])\n",
        "      if pitch == 'br':\n",
        "        note += dur\n",
        "      else:\n",
        "        for i in range(dur):\n",
        "          while note + i >= mat.shape[0]:\n",
        "            mat = np.append(mat, np.full([1, 128], False), axis = 0)\n",
        "          mat[note + i, int(pitch)] = True\n",
        "  return mat\n",
        "\n",
        "track = pypianoroll.BinaryTrack(pianoroll=token_to_music(string)) #fill in \"\" with tokens \n",
        "                                \n",
        "pypianoroll.write(os.path.join(root_dir, \"test_output3.mid\"), pypianoroll.Multitrack(resolution = 8, tempo = np.full([200, 1], 120), tracks = [track]))\n",
        "\n",
        "token_to_music(string)"
      ],
      "metadata": {
        "id": "1iWlTnydkpSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3bb45a-4484-49aa-88b8-a663f00fdf53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       ...,\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    }
  ]
}